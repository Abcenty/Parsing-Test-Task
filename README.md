# Документация выполненного тестового задания и инструкция по запуску (формулировка задания в конце)

Зависимости находятся в файле poetry.lock

Для установки переменных окружения создайте файл .env в той же папке, что и файл .env.example и скопируйте
содержимое .env.example в файл .env (добавил файл с переменными окружения в .gitignore ради соблюдения формальности)

Для начала работы парсера запустите файл main.py

Для удобства объяснения работы программы, залогировал ключевые моменты работы

Точкой входа в программу является метод run_parser

В начале произойдет создание 3х задач на отправку дублирующих запросов

После отправки 3 запросов событийный цикл начнет опрашивать эти задачи по очереди

Когда задача успешно притянет данные, лог об этом выведется в консоль, после чего будет создана
задача на добавление этих данных через метод add_data, контроль потоком перейдет этой корутине

add_data сымитирует работу, после чего последует отмена дублирующих запросов и run_parser уйдет в сон,
закончив событийный цикл

Событийный цикл повторит описанную выше работу благодаря бесконечному циклу внутри run_parser


Итого по требованиям:
1) Данные запрошены через httpx
2) run_parser завершает работу уходом в сон на секунду, что обеспечивает временной интервал между запросами в 1 секунду
3) Ошибка работы при падении запросов логируется и выводится в консоль
4) Полученные с запрсоа данные отправляются в хранилище методом add_data
5) Данные отправляются в необработанном текстовом виде

Доп требования:
1) Метод add_data выполняется параллельно за счет создания отдельной задачи
2) Для минимизации риска падения запросов отправляются 3 дублирующих запроса
3) DataStorage.send_data остался в неизменном виде, расширение DataStorage не потребовалось


Пример логов результата работы событийного цикла:
main.py:32 #INFO     [2024-07-15 05:25:43,310] - __main__ - Создал дублирующие задачи
main.py:15 #INFO     [2024-07-15 05:25:43,330] - __main__ - Отправляю запрос
main.py:15 #INFO     [2024-07-15 05:25:43,360] - __main__ - Отправляю запрос
main.py:15 #INFO     [2024-07-15 05:25:43,373] - __main__ - Отправляю запрос
_client.py:1773 #INFO     [2024-07-15 05:25:43,644] - httpx - HTTP Request: GET https://iss.moex.com/iss/history/engines/stock/markets/shares/securities/MOEX.xml "HTTP/1.1 200 OK"
main.py:17 #INFO     [2024-07-15 05:25:43,649] - __main__ - Притянул данные
main.py:24 #INFO     [2024-07-15 05:25:43,651] - __main__ - Создал задачу добавления в хранилище
main.py:36 #INFO     [2024-07-15 05:25:43,652] - __main__ - Отменил дублирующую задачу
main.py:36 #INFO     [2024-07-15 05:25:43,653] - __main__ - Отменил дублирующую задачу
main.py:19 #INFO     [2024-07-15 05:25:43,654] - __main__ - Задача отменена
main.py:19 #INFO     [2024-07-15 05:25:43,657] - __main__ - Задача отменена


Спасибо за интересное тестовое задание :)


___________________________________________________________________________________________________________________________________________
# Parsing-Test-Task

## Парсинг Мосбиржи

Данное тестовое задание предназначено для проверки ваших скиллов в работе с модулем asyncio.

Задача:
Реализовать асинхронное получение данных по ссылке: https://iss.moex.com/iss/history/engines/stock/markets/shares/securities/MOEX.xml

Требования:
- Для http запросов использовать библиотеку httpx https://pypi.org/project/httpx/
- Запросы должны отправляться 1 раз в секунду
- Требуется предусмотреть обработку ошибок при падении запросов
- Полученные данные складывать в DataStorage, предусмотрен метод add_data
- Как либо предварительно обрабатывать данные не требуется, просто отправляйте текст из ответа запроса


Для распараллеливания задач использовать исключительно асинхронность(asyncio)



Бонусная часть:
- В DataStorage.add_data предусмотрен asyncio.sleep на 1 секунду: выполнять метод add_data на фоне что-бы он не тормозил парсер
- Минимизировать риск падения запросов, ниже вариант решения на дублирующих запросах

Дублирующие запросы: параллельная отправка двух или более запросов для получения одних и тех же данных, они не ждут друг друга, а отменяют остальные запросы при получении первого валидного ответа одним из запросов.



P.S. Про дублирующие запросы для чего они нужны: В нашей работе часто возникают случаи когда падают запросы(кривые прокси, баны и т.д.)
и что-бы это минимизизровать мы отправляем дублирующие запросы.
В данном случае шансов падения сразу всех запросов куда ниже.

Если хоть один запрос прошел: не снижается скорость доставки данных. Но за дублирующие запросы мы расплачиваемся ресурсами(CPU, Network)

P.S.S запрещается как либо модифицировать метод DataStorage.send_data, вы можете расширить DataStorage без вреда текущему функционалу
